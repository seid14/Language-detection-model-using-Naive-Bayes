{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNp9kV1320jVI48rW1+tcVR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seid14/Language-detection-model-using-Naive-Bayes/blob/main/Language_detection_model_using_Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing important packages"
      ],
      "metadata": {
        "id": "9OhRQmkjmVEu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofutT56AlE_2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mouting Drive"
      ],
      "metadata": {
        "id": "Z22E-OsIapaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3noDkfWmRzw",
        "outputId": "6fdc8419-8bec-4056-cf15-2eebf5ae4332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading Data"
      ],
      "metadata": {
        "id": "W3IVICOQa1FX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Datasets/oromo_sentences.csv')\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/Datasets/somali_sentences.csv')\n",
        "df3 = pd.read_csv('/content/drive/MyDrive/Datasets/swahili_sentences.csv')"
      ],
      "metadata": {
        "id": "OjBuY5NumtoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concatenate all dataframes into one"
      ],
      "metadata": {
        "id": "uqkSrz6_a-6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = df.append([df2,df3], ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrtzfG75bDTD",
        "outputId": "b5052252-4041-481d-a94b-b38212eb3cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-118-b3a8ab2aae8b>:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  dataset = df.append([df2,df3], ignore_index=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data inspection and insigh"
      ],
      "metadata": {
        "id": "Oyhld66ZbKG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.info()"
      ],
      "metadata": {
        "id": "Uc4tQW8h0AtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.language.value_counts()"
      ],
      "metadata": {
        "id": "W7F90Qmn0Cop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[dataset.language =='somali'].sample(10)"
      ],
      "metadata": {
        "id": "9WFkYRfyzjMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "YiRvZMOBbZWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = dataset['text']"
      ],
      "metadata": {
        "id": "UT3Igvvrzwa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=dataset['language']"
      ],
      "metadata": {
        "id": "T0Yatzuez0ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now split our dataset into training and test sets and start working with the models."
      ],
      "metadata": {
        "id": "Ji8KKFS94y5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=100)"
      ],
      "metadata": {
        "id": "etKmE_zM4sZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we have to call the CountVectorizer object of sklearn in order to create, for each char bigram, the number of its occurrences in each sentence. Then we can create the pipeline that vectorizes our data and gives it to the model, which is a Multinomial Naive Bayes."
      ],
      "metadata": {
        "id": "2qa1bUAo4UTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnt = CountVectorizer(analyzer = 'char',ngram_range=(2,2))\n",
        "\n",
        "model = pipeline = Pipeline([\n",
        "   ('vectorizer',cnt),  \n",
        "   ('model',MultinomialNB())\n",
        "])"
      ],
      "metadata": {
        "id": "Wd9fBx274Ofh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "D0PFG-8nAFWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now fit our pipeline and calculate predictions on the test set:"
      ],
      "metadata": {
        "id": "jt7LWWJ34ZPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(x_test)\n",
        "accuracy = accuracy_score(y_test,y_pred)\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "print(\"Accuracy is :\",accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofFqMTTi__b8",
        "outputId": "f94a9523-e971-40cf-c4e6-a9a545cad21f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is : 0.9996689289852674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can finally take a look at the confusion matrix:"
      ],
      "metadata": {
        "id": "0MDv_wqb5ofB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))"
      ],
      "metadata": {
        "id": "rch4zTAi5qc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "H_AMODQ76WX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking on the model"
      ],
      "metadata": {
        "id": "VV-ifm0dAzfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "VD4mAR7sA8p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text):\n",
        "    lang = model.predict([text])\n",
        "    print(\"Language is: \", lang[0])"
      ],
      "metadata": {
        "id": "y4utFUmbA-z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text =input(\"Enter a text here:\")\n",
        "predict(text)"
      ],
      "metadata": {
        "id": "ySgpdE05BBFK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}